# SparkRTC

SparkRTC is built on WebRTC.
The major difference is the goal -- SparkRTC is designed for ultra low latency, with coordinations across a series of modules in WebRTC.
SparkRTC follows the license of WebRTC, and is open-source, free to use, and only for research purpose.

**WebRTC is a free, open software project** that provides browsers and mobile
applications with Real-Time Communications (RTC) capabilities via simple APIs.
The WebRTC components have been optimized to best serve this purpose.

**Our mission:** To enable rich, high-quality RTC applications to be
developed for the browser, mobile platforms, and IoT devices, and allow them
all to communicate via a common set of protocols.

The WebRTC initiative is a project supported by Google, Mozilla and Opera,
amongst others.

## Before You Start

First, be sure to install the [prerequisite software](https://webrtc.github.io/webrtc-org/native-code/development/prerequisite-sw/).

Generally, we need to install the [depot_tools](https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/depot_tools_tutorial.html#_setting_up).
```
git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git
export PATH=/path/to/depot_tools:$PATH
```


## Getting the Code

For desktop development:

Clone the current repo:

```
git clone https://github.com/hkust-spark/sparkrtc-public.git
```

Enter the root directory of the repo:

```
cd ./sparkrtc-public
```

Clone the submodules:

```
git submodule update --init --recursive
```

Sync with other WebRTC-related repos using the `gclient` tool installed before. This may take 10 to 20 minutes, depending on the network speed.

```
gclient sync
```

NOTICE: During your first sync, you’ll have to accept the license agreement of the Google Play Services SDK.

The checkout size is large due the use of the Chromium build toolchain and many dependencies. 

## Generating Ninja project files

[Ninja](https://ninja-build.org/) is the default build system for all platforms.
[Ninja](https://ninja-build.org/) project files are generated using [GN](https://gn.googlesource.com/gn/+/master/README.md). They're put in a directory of your choice, like `out/Debug`, but you can use any directory for keeping multiple configurations handy.

To generate project files using the defaults (Debug build), run (in the root directory of the repo):

```
gn gen out/Default
```

See the [GN](https://gn.googlesource.com/gn/+/master/README.md) documentation for all available options. 

## Compiling

When you have Ninja project files generated (see previous section), compile using:

For Ninja project files generated in out/Default:

```
ninja -C out/Default
```

## Example Applications

WebRTC contains several example applications, which can be found under `src/webrtc/examples` and `src/talk/examples`. Higher level applications are listed first.

Peerconnection consist of two applications using the WebRTC Native APIs:

* A server application, with target name `peerconnection_server`

* A client application, with target name `peerconnection_client` (not currently supported on Mac/Android)

The client application has simple voice and video capabilities. The server enables client applications to initiate a call between clients by managing signaling messages generated by the clients.

**Setting up P2P calls between peerconnection_clients:** 
Start peerconnection_server. You should see the following message indicating that it is running:

```
Server listening on port 8888
```

Start any number of `peerconnection_clients` and connect them to the server. The client UI consists of a few parts:

**Connecting to a server:** When the application is started you must specify which machine (by IP address) the server application is running on. Once that is done you can press Connect or the return button.

**Select a peer:** Once successfully connected to a server, you can connect to a peer by double-clicking or select+press return on a peer’s name.

**Video chat:** When a peer has been successfully connected to, a video chat will be displayed in full window.

**Ending chat session:** Press Esc. You will now be back to selecting a peer.

**Ending connection:** Press Esc and you will now be able to select which server to connect to.

For more guidelines, see [here](https://webrtc.github.io/webrtc-org/native-code/development/).

# Local Video Guidelines

We implemented a ``peerconnection_localvideo`` example modified from ``peerconnection_client`` on MacOS and Linux for testing purposes. It streams s local video sequence (YUV420) instead of capturing from cameras. The GUI is optionally removed for command line testing.


## CLI Options:

1. Start peerconnection_server

```
./peerconnection_server
```

2. Start the receiver with the filename of the received yuv file.

```
./peerconnection_localvideo --recon "recon.yuv"
```


By default, the GUI is turned off. Add ```--gui``` on receiver to open the rendered view:

```
./peerconnection_localvideo --gui --recon "recon.yuv"
```

If it's running on Windows system, the parameter need to use '=' to connect, e.g.
```
./peerconnection_localvideo --gui --recon="recon.yuv"
```

3. Start the sender(Should after receiver) with information of yuv file to be streamed.

```
./peerconnection_localvideo --file "input.yuv" --height 1080 --width 1920 --fps 24
```

# Experimenting Testbed Setup Guide

## Preparations

All prerequisites of the SparkRTC are needed.

Also,

download and install  `ffmpeg`, and if using Linux, download `mahimahi`.

```bash
git clone https://github.com/LW945/mahimahi.git
cd mahimahi
./configure
make
make install
```

Install cv2

```bash
pip install opencv-python 
pip install opencv-contrib-python
```

### **Experiment Code**

The experiment code is located in the `sparkrtc/my_experiment` directory.

### Directory Structure

After downloading the repo, create the following directories in `sparkrtc/my_experiment` manually:

```bash
mkdir -p data file send fig/(video_name)
```

Replace the `(video_name)` with the name of your video. 

### **Sending Video**

The video to be sent should be placed in the data directory. And the video should be in `.yuv` format.

For example:

`sparkrtc/my_experiment/data/video_0a86_qrcode.yuv`

### **Picture Frames**

The pictures in the `sparkrtc/my_experiment/send/(video_name)` directory are all frames from the send video.

Don't need to generate this seperately, the script can handle this.

For example, add all frames of `video_0a86_qrcode.yuv` to

`sparkrtc/my_experiment/send/video_0a86`

### **Download WeChat QR Code Scanner Model**

Some experiments include embedding QR codes into frames, and the WeChat QR Code Scanner Model is needed besides the cv2 library.

Download the WeChat QR code scanner model file from GitHub:

https://github.com/Tianxiaomo/qrdecoder/tree/master/model

Put every `detect*, sr*` files in `sparkrtc/my_experiment/code/`.

### **Path Configuration**

You need to manually change the path of your installation of `ffmpeg` and `mahimahi` in `sparkrtc/my_experiment/code/process_video_qrcode.py`

Also, you can setup the video`fps` here.

```shell
import os
import qrcode
import cv2
import argparse
import matplotlib.pyplot as plt
import numpy as np
import subprocess
import signal
import time
from concurrent.futures import ThreadPoolExecutor
ffmpeg_path = "your/path/to/ffmpeg"
mahimahi_path = "your/path/to/mahimahi"
fps = 30
```

### **Loss Configuration**

You need to manually change the experiment loss configuration of mahimahi via a config file. 

Create a file named loss_trace in `sparkrtc/my_experiment/file`

Config the file with the formate of 

```
timestamp(in mm), loss rate
timestamp(in mm), loss rate 
```

For example:

```
3470,0
3500,0.1
```

## One-tap Experiments

Using `sparkrtc/my_experiment/code/run.sh` can run several experiments at once to make the process easier.

**run.sh Usage**

First generate video with qrcode.
```bash
./run.sh -i <data>.yuv -p gen_send_video
# For example
./run.sh -i video_0a86_qrcode.yuv  -p gen_send_video
```

Then run the send program.
```bash
./run.sh -i <data>.yuv -p all
# For example
./run.sh -i video_0a86_qrcode.yuv  -p all
```

### Detailed Usage

The script - process_video_qrcode.py - can be run with different options to perform various tasks. The options are:

1. gen_send_video: Generate video with QR codes.
2. decode_recv_video: Decode received video and calculate metrics.
3. show_fig: Generate figures from experiment results.
4. send_and_recv: Send video and receive it, then process the results.

#### **Generating Video with QR Codes**

To generate a video with QR codes embedded in the frames:

```bash
python3 process_video_qrcode.py --option=gen_send_video --data=<video_name>
```

#### **Decoding Received Video**

To decode the received video and calculate SSIM and delay:

```bash
python3 process_video_qrcode.py --option=decode_recv_video --data=<video_name>
```
#### **Generating Figures**

To generate figures based on the experiment results:

```bash
python3 process_video_qrcode.py --option=show_fig --data=<video_name>
```
#### **Sending and Receiving Video**

To send a video and receive it, then process the received video:

```bash
python process_video_qrcode.py --option=send_and_recv --data=<video_name>
```

## Output Results

**General Output Structure**

The output results from the experiments are generally stored in the following directory structure:

```bash
project_root/my_experiment/
├── data
│   ├── <video_name>.yuv
│   └── <video_name>_qrcode.yuv
├── file
|   ├── loss_trace
│   └── trace_logs
│       └── <bandwidth_trace>.log
├── qrcode
│   └── <video_name>
│       └── qrcode_<number>.png
├── result
│   └── <bandwidth_trace>
│       └── output_1
│           ├── fig
│           │   └── <video_name>
│           │       ├── delay.png
│           │       └── ...
│           ├── rec
│           │   └── <video_name>
│           │       ├── end_stamp.log
│           │       ├── rate_timestamp.log
│           │       ├── raw_frames
│           │       │   └── frame<frame_number>.png
│           │       ├── recon.yuv
│           │       ├── recv.log
│           │       ├── send.log
│           │       └── start_stamp.log
│           └── res
│               └── <video_name>
│                   ├── delay.log
│                   ├── frame_size.log
│                   ├── psnr
│                   │   └── psnr.log
│                   ├── rate.log
│                   └── ssim
│                       └── ssim.log
└── send
    └── <video_name>
        └── frame<frame_number>.png
```

#### **Output Formats**

**Logs and Metrics**

 1. **SSIM Log** (`ssim.log`):

    •	Directory: `res/<video_name>/ssim/`

​	•	Format: frame_number, ssim_value

​	•	Description: Contains SSIM values for each frame.

2. **Delay Log (**`delay.log`):

   •	Directory: `res/<video_name>/`

​	•	Format: delay_value

​	•	Description: Contains delay values for each frame.


## *Appendix: **Detailed Description***

##### **QR Code Generation**

`gen_qrcode(cfg, num)` generates QR codes and saves them as images. It then creates a YUV video from these images using ffmpeg.

**Overlaying QR Codes on Video**

`overlay_qrcode_to_video(cfg, num)` overlays the generated QR codes onto the video frames at specified positions.

**Decoding QR Codes from Video**

`scan_qrcode_fast(frame_list, qrcode_raw_dir, qrcode_res_dir)` and `scan_qrcode(num, qrcode_raw_dir, qrcode_res_dir)` decode QR codes from video frames and log the results.

**Calculating SSIM**

`cal_ssim_fast(frame_list, send_image_path, ssim_res_path, qrcode_res_path)` and `cal_ssim(num, send_image_path, ssim_res_path, qrcode_res_path)` calculate the SSIM between sent and received video frames.

**Calculating Delay**

`cal_delay(recv_dir)` calculates the delay between sent and received frames based on timestamps.

**Sending and Receiving Video**

`send_and_recv_video(cfg, num)` handles the process of sending the video with QR codes and receiving the video, then processes the received video to calculate SSIM and delay.

**Plotting Results**

`show_experiment_fig(cfg, fig_dir)` generates and saves plots showing SSIM loss and load time based on the experiment results.
